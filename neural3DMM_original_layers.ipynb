{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX TITAN X\n",
      "GeForce GTX TITAN X\n",
      "GeForce GTX TITAN X\n",
      "GeForce GTX TITAN X\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import mesh_sampling\n",
    "import trimesh\n",
    "from shape_data import ShapeData\n",
    "\n",
    "from autoencoder_dataset import cached_autoencoder_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from spiral_utils import get_adj_trigs, generate_spirals\n",
    "from models import SpiralAutoencoder\n",
    "from train_funcs import train_autoencoder_dataloader\n",
    "from test_funcs import test_autoencoder_dataloader\n",
    "\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "meshpackage = 'mpi-mesh' # 'mpi-mesh', 'trimesh'\n",
    "root_dir = '/home/jingwang/Data/data/'\n",
    "\n",
    "dataset = 'FaceWarehouse'\n",
    "name = 'try_variational'\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "GPU = True\n",
    "device_idx = 0 # 0, 1, 2, 3\n",
    "device_ids = [0, 1, 2, 3]\n",
    "for idx in device_ids:\n",
    "    print(torch.cuda.get_device_name(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "generative_model = 'autoencoder'\n",
    "downsample_method = 'COMA_downsample' # choose'COMA_downsample' or 'meshlab_downsample'\n",
    "\n",
    "\n",
    "# below are the arguments for the DFAUST run\n",
    "reference_mesh_file = os.path.join(root_dir, dataset, 'template', 'template.obj')\n",
    "downsample_directory = os.path.join(root_dir, dataset,'template', downsample_method)\n",
    "ds_factors = [4, 4, 4, 4]\n",
    "step_sizes = [2, 2, 1, 1, 1]\n",
    "filter_sizes_enc = [[3, 16, 32, 64, 128],[[],[],[],[],[]]]\n",
    "filter_sizes_dec = [[128, 64, 32, 32, 16],[[],[],[],[],3]]\n",
    "dilation_flag = True\n",
    "if dilation_flag:\n",
    "    dilation=[2, 2, 1, 1, 1] \n",
    "else:\n",
    "    dilation = None\n",
    "reference_points = [[5930]]# [[3567,4051,4597]] # [[414]]  # used for COMA with 3 disconnected components\n",
    "\n",
    "args = {'generative_model': generative_model,\n",
    "        'name': name, 'data': os.path.join(root_dir, dataset, 'preprocessed',name),\n",
    "        'results_folder':  os.path.join(root_dir, dataset,'results/spirals_'+ generative_model),\n",
    "        'reference_mesh_file':reference_mesh_file, 'downsample_directory': downsample_directory,\n",
    "        'checkpoint_file': 'checkpoint',\n",
    "        'seed':2, 'loss':'l1',\n",
    "        'batch_size': 16, 'num_epochs':300, 'eval_frequency':200, 'num_workers': 40,\n",
    "        'filter_sizes_enc': filter_sizes_enc, 'filter_sizes_dec': filter_sizes_dec,\n",
    "        'nz': 128, # 100 identity + 46 expression \n",
    "        'ds_factors': ds_factors, 'step_sizes' : step_sizes, 'dilation': dilation,\n",
    "        \n",
    "        'lr':1e-3, \n",
    "        'regularization': 5e-5,         \n",
    "        'scheduler': True, 'decay_rate': 0.99,'decay_steps':1,  \n",
    "        'resume': True,\n",
    "        \n",
    "        'mode':'train', 'shuffle': True, 'nVal': 100, 'normalization': True,\n",
    "        'write_mesh': True,\n",
    "        'lambda_var': 0.5,\n",
    "        'worst_face_num': 20,\n",
    "        'use_cache': True}\n",
    "\n",
    "args['results_folder'] = os.path.join(args['results_folder'],'latent_'+str(args['nz']))\n",
    "    \n",
    "if not os.path.exists(os.path.join(args['results_folder'])):\n",
    "    os.makedirs(os.path.join(args['results_folder']))\n",
    "\n",
    "summary_path = os.path.join(args['results_folder'],'summaries',args['name'])\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)  \n",
    "    \n",
    "checkpoint_path = os.path.join(args['results_folder'],'checkpoints', args['name'])\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "    \n",
    "samples_path = os.path.join(args['results_folder'],'samples', args['name'])\n",
    "if not os.path.exists(samples_path):\n",
    "    os.makedirs(samples_path)\n",
    "    \n",
    "prediction_path = os.path.join(args['results_folder'],'predictions', args['name'])\n",
    "if not os.path.exists(prediction_path):\n",
    "    os.makedirs(prediction_path)\n",
    "\n",
    "if not os.path.exists(downsample_directory):\n",
    "    os.makedirs(downsample_directory)\n",
    "\n",
    "downsample_mesh_path = os.path.join(args['results_folder'],'downsample_mesh', args['name'])\n",
    "if not os.path.exists(downsample_mesh_path):\n",
    "    os.makedirs(downsample_mesh_path)\n",
    "    \n",
    "worst_mesh_path = os.path.join(args['results_folder'],'worst_test', args['name'])\n",
    "if not os.path.exists(downsample_mesh_path):\n",
    "    os.makedirs(downsample_mesh_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data .. \n",
      "Loading Transform Matrices ..\n",
      "Calculating reference points for downsampled versions..\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(args['seed'])\n",
    "print(\"Loading data .. \")\n",
    "if not os.path.exists(args['data']+'/mean.npy') or not os.path.exists(args['data']+'/std.npy'):\n",
    "    shapedata =  ShapeData(nVal=args['nVal'], \n",
    "                          train_file=args['data']+'/train.npy', \n",
    "                          test_file=args['data']+'/test.npy', \n",
    "                          reference_mesh_file=args['reference_mesh_file'],\n",
    "                          normalization = args['normalization'],\n",
    "                          meshpackage = meshpackage, load_flag = True)\n",
    "    np.save(args['data']+'/mean.npy', shapedata.mean)\n",
    "    np.save(args['data']+'/std.npy', shapedata.std)\n",
    "else:\n",
    "    shapedata = ShapeData(nVal=args['nVal'], \n",
    "                         train_file=args['data']+'/train.npy',\n",
    "                         test_file=args['data']+'/test.npy', \n",
    "                         reference_mesh_file=args['reference_mesh_file'],\n",
    "                         normalization = args['normalization'],\n",
    "                         meshpackage = meshpackage, load_flag = False)\n",
    "    shapedata.mean = np.load(args['data']+'/mean.npy')\n",
    "    shapedata.std = np.load(args['data']+'/std.npy')\n",
    "    shapedata.n_vertex = shapedata.mean.shape[0]\n",
    "    shapedata.n_features = shapedata.mean.shape[1]\n",
    "\n",
    "if not os.path.exists(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl')):\n",
    "    if shapedata.meshpackage == 'trimesh':\n",
    "        raise NotImplementedError('Rerun with mpi-mesh as meshpackage')\n",
    "    print(\"Generating Transform Matrices ..\")\n",
    "    if downsample_method == 'COMA_downsample':\n",
    "        M,A,D,U,F = mesh_sampling.generate_transform_matrices(shapedata.reference_mesh, args['ds_factors'])\n",
    "        if args['write_mesh']:\n",
    "            import openmesh\n",
    "            for i in range(len(M)):\n",
    "                mesh = openmesh.TriMesh(points=M[i].v,face_vertex_indices=M[i].f)\n",
    "                openmesh.write_mesh(os.path.join(args['results_folder'],'downsample_mesh',args['name'], '%d.obj'%i),mesh)\n",
    "    with open(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl'), 'wb') as fp:\n",
    "        M_verts_faces = [(M[i].v, M[i].f) for i in range(len(M))]\n",
    "        pickle.dump({'M_verts_faces':M_verts_faces,'A':A,'D':D,'U':U,'F':F}, fp)\n",
    "else:\n",
    "    print(\"Loading Transform Matrices ..\")\n",
    "    with open(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl'), 'rb') as fp:\n",
    "        #downsampling_matrices = pickle.load(fp,encoding = 'latin1')\n",
    "        downsampling_matrices = pickle.load(fp)\n",
    "            \n",
    "    M_verts_faces = downsampling_matrices['M_verts_faces']\n",
    "    if shapedata.meshpackage == 'mpi-mesh':\n",
    "        from psbody.mesh import Mesh\n",
    "        M = [Mesh(v=M_verts_faces[i][0], f=M_verts_faces[i][1]) for i in range(len(M_verts_faces))]\n",
    "    elif shapedata.meshpackage == 'trimesh':\n",
    "        M = [trimesh.base.Trimesh(vertices=M_verts_faces[i][0], faces=M_verts_faces[i][1], process = False) for i in range(len(M_verts_faces))]\n",
    "    A = downsampling_matrices['A']\n",
    "    D = downsampling_matrices['D']\n",
    "    U = downsampling_matrices['U']\n",
    "    F = downsampling_matrices['F']\n",
    "        \n",
    "# Needs also an extra check to enforce points to belong to different disconnected component at each hierarchy level\n",
    "print(\"Calculating reference points for downsampled versions..\")\n",
    "for i in range(len(args['ds_factors'])):\n",
    "    if shapedata.meshpackage == 'mpi-mesh':\n",
    "        dist = euclidean_distances(M[i+1].v, M[0].v[reference_points[0]])\n",
    "    elif shapedata.meshpackage == 'trimesh':\n",
    "        dist = euclidean_distances(M[i+1].vertices, M[0].vertices[reference_points[0]])\n",
    "    reference_points.append(np.argmin(dist,axis=0).tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spiral generation for hierarchy 0 (11510 vertices) finished\n",
      "spiral generation for hierarchy 1 (2878 vertices) finished\n",
      "spiral generation for hierarchy 2 (720 vertices) finished\n",
      "spiral generation for hierarchy 3 (180 vertices) finished\n",
      "spiral generation for hierarchy 4 (45 vertices) finished\n",
      "spiral sizes for hierarchy 0:  14\n",
      "spiral sizes for hierarchy 1:  13\n",
      "spiral sizes for hierarchy 2:  9\n",
      "spiral sizes for hierarchy 3:  9\n",
      "spiral sizes for hierarchy 4:  9\n"
     ]
    }
   ],
   "source": [
    "if shapedata.meshpackage == 'mpi-mesh':\n",
    "    sizes = [x.v.shape[0] for x in M]\n",
    "elif shapedata.meshpackage == 'trimesh':\n",
    "    sizes = [x.vertices.shape[0] for x in M]\n",
    "Adj, Trigs = get_adj_trigs(A, F, shapedata.reference_mesh, meshpackage = shapedata.meshpackage)\n",
    "\n",
    "spirals_np, spiral_sizes,spirals = generate_spirals(args['step_sizes'], \n",
    "                                                    M, Adj, Trigs, \n",
    "                                                    reference_points = reference_points, \n",
    "                                                    dilation = args['dilation'], random = False, \n",
    "                                                    meshpackage = shapedata.meshpackage, \n",
    "                                                    counter_clockwise = True)\n",
    "\n",
    "bU = []\n",
    "bD = []\n",
    "for i in range(len(D)):\n",
    "    d = np.zeros((1,D[i].shape[0]+1,D[i].shape[1]+1))\n",
    "    u = np.zeros((1,U[i].shape[0]+1,U[i].shape[1]+1))\n",
    "    d[0,:-1,:-1] = D[i].todense()\n",
    "    u[0,:-1,:-1] = U[i].todense()\n",
    "    d[0,-1,-1] = 1\n",
    "    u[0,-1,-1] = 1\n",
    "    bD.append(d)\n",
    "    bU.append(u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "if GPU:\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cuda:\"+str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "tspirals = [torch.from_numpy(s).long().to(device) for s in spirals_np]\n",
    "tD = [torch.from_numpy(s).float().to(device) for s in bD]\n",
    "tU = [torch.from_numpy(s).float().to(device) for s in bU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# Building model, optimizer, and loss function\n",
    "\n",
    "if args['use_cache']:\n",
    "    dataset_train = cached_autoencoder_dataset(root_dir = args['data'], points_dataset = 'train',\n",
    "                                               shapedata = shapedata,\n",
    "                                               normalization = args['normalization'], device=device)\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=args['batch_size'],\\\n",
    "                                         shuffle = args['shuffle'], num_workers=0)\n",
    "else:\n",
    "    dataset_train = autoencoder_dataset(root_dir = args['data'], points_dataset = 'train',\n",
    "                                               shapedata = shapedata,\n",
    "                                               normalization = args['normalization'])\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=args['batch_size'],\\\n",
    "                                         shuffle = args['shuffle'], num_workers = args['num_workers'],pin_memory=True)\n",
    "\n",
    "if args['use_cache']:\n",
    "    dataset_val = cached_autoencoder_dataset(root_dir = args['data'], points_dataset = 'val', \n",
    "                                             shapedata = shapedata,\n",
    "                                             normalization = args['normalization'], device=device)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=args['batch_size'],\\\n",
    "                                         shuffle = False, num_workers=0)\n",
    "else:\n",
    "    dataset_val = autoencoder_dataset(root_dir = args['data'], points_dataset = 'val', \n",
    "                                             shapedata = shapedata,\n",
    "                                             normalization = args['normalization'])\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=args['batch_size'],\\\n",
    "                                         shuffle = False, num_workers = args['num_workers'],pin_memory=True)\n",
    "\n",
    "\n",
    "if 'autoencoder' in args['generative_model']:\n",
    "        model = SpiralAutoencoder(filters_enc = args['filter_sizes_enc'],   \n",
    "                                  filters_dec = args['filter_sizes_dec'],\n",
    "                                  latent_size=args['nz'],\n",
    "                                  sizes=sizes,\n",
    "                                  spiral_sizes=spiral_sizes,\n",
    "                                  spirals=tspirals,\n",
    "                                  D=tD, U=tU).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "#             model = torch.nn.parallel.DataParallel(model, device_ids=device_ids)\n",
    "            print('Let\\'s use %d GPUs!'%torch.cuda.device_count())\n",
    " \n",
    "    \n",
    "optim = torch.optim.Adam(model.parameters(),lr=args['lr'],weight_decay=args['regularization'])\n",
    "if args['scheduler']:\n",
    "    scheduler=torch.optim.lr_scheduler.StepLR(optim, args['decay_steps'],gamma=args['decay_rate'])\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "if args['loss']=='l1':\n",
    "    def loss_l1(outputs, targets):\n",
    "        L = torch.abs(outputs - targets).mean()\n",
    "        return L \n",
    "    loss_fn = loss_l1\n",
    "elif arg['loss']=='l1_var':\n",
    "    lambda_var = args['lambda_var']\n",
    "    def variational_loss(tx,tx_hat):\n",
    "        x,mu,logvar = tx\n",
    "        l1_loss = torch.mean(torch.abs(x-tx))\n",
    "        var_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return l1_loss + lambda_var * var_loss\n",
    "    loss_fn = variational_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters is: 1726531\n",
      "SpiralAutoencoder(\n",
      "  (conv): ModuleList(\n",
      "    (0): SpiralConv(\n",
      "      (conv): Linear(in_features=42, out_features=16, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (1): SpiralConv(\n",
      "      (conv): Linear(in_features=208, out_features=32, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (2): SpiralConv(\n",
      "      (conv): Linear(in_features=288, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (3): SpiralConv(\n",
      "      (conv): Linear(in_features=576, out_features=128, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (fc_latent_enc): Linear(in_features=5888, out_features=128, bias=True)\n",
      "  (fc_latent_dec): Linear(in_features=128, out_features=5888, bias=True)\n",
      "  (dconv): ModuleList(\n",
      "    (0): SpiralConv(\n",
      "      (conv): Linear(in_features=1152, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (1): SpiralConv(\n",
      "      (conv): Linear(in_features=576, out_features=32, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (2): SpiralConv(\n",
      "      (conv): Linear(in_features=416, out_features=32, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (3): SpiralConv(\n",
      "      (conv): Linear(in_features=448, out_features=16, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (4): SpiralConv(\n",
      "      (conv): Linear(in_features=224, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters is: {}\".format(params)) \n",
    "print(model)\n",
    "# print(M[4].v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint from file /home/jingwang/Data/data/FaceWarehouse/results/spirals_autoencoder/latent_128/checkpoints/try_variational/checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/580 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [06:02<00:00,  1.60it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | tr 0.0736729741931 | val 0.0989421790838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 | tr 0.0740792580463 | val 0.105587059259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 | tr 0.0704835960963 | val 0.0953730034828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | tr 0.0694200358386 | val 0.0922196090221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 | tr 0.0677045895879 | val 0.0944224008918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 | tr 0.0668770822343 | val 0.0945045202971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 | tr 0.0674639995306 | val 0.0939065256715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 | tr 0.0658795791646 | val 0.0882619777322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 | tr 0.0652708638138 | val 0.0951713106036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 | tr 0.065407196214 | val 0.084745477736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 | tr 0.0627900400938 | val 0.0873714980483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 | tr 0.061959224631 | val 0.093751963377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 | tr 0.0626770003538 | val 0.0848408234119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 | tr 0.0614681518026 | val 0.0866807445884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | tr 0.0606260926944 | val 0.0937373268604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 | tr 0.0605161878698 | val 0.0919766399264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 | tr 0.0601327626227 | val 0.0935595461726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 | tr 0.0587995279388 | val 0.0830590614676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 | tr 0.0595444848311 | val 0.0820716997981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35 | tr 0.0584011783695 | val 0.0879223880172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36 | tr 0.057937136787 | val 0.0841301360726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 | tr 0.0571713261884 | val 0.0829522076249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 | tr 0.0563189331314 | val 0.0817415553331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39 | tr 0.0568614152806 | val 0.088312112391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 | tr 0.0557321260908 | val 0.0832271012664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41 | tr 0.0556383272055 | val 0.0858266353607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42 | tr 0.0552187342839 | val 0.0851809173822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43 | tr 0.0549155903402 | val 0.0841249370575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44 | tr 0.0541788438849 | val 0.0803450864553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45 | tr 0.0537255445816 | val 0.0832906001806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46 | tr 0.0531289971466 | val 0.0840942344069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47 | tr 0.0535769395081 | val 0.0787072440982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48 | tr 0.0532942036488 | val 0.0824783706665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49 | tr 0.052690090496 | val 0.0776695096493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 | tr 0.0522212033834 | val 0.0799286228418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51 | tr 0.0519565350537 | val 0.0794578287005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52 | tr 0.0517453294376 | val 0.0819175854325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53 | tr 0.0512162586674 | val 0.0809000092745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54 | tr 0.0515216049899 | val 0.0818503040075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55 | tr 0.0507059218812 | val 0.076242839694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56 | tr 0.0502552493315 | val 0.0773185184598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57 | tr 0.0497233957695 | val 0.0803504699469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [05:53<00:00,  1.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58 | tr 0.0500977822033 | val 0.0835678943992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 565/580 [05:44<00:09,  1.64it/s]"
     ]
    }
   ],
   "source": [
    "if args['mode'] == 'train':\n",
    "    writer = SummaryWriter(summary_path)\n",
    "    with open(os.path.join(args['results_folder'],'checkpoints', args['name'] +'_params.json'),'w') as fp:\n",
    "        saveparams = copy.deepcopy(args)\n",
    "        json.dump(saveparams, fp)\n",
    "        \n",
    "    if args['resume']:\n",
    "            print('loading checkpoint from file %s'%(os.path.join(checkpoint_path,args['checkpoint_file'])))\n",
    "            checkpoint_dict = torch.load(os.path.join(checkpoint_path,args['checkpoint_file']+'.pth.tar'),map_location=device)\n",
    "            start_epoch = checkpoint_dict['epoch'] + 1\n",
    "            model.load_state_dict(checkpoint_dict['autoencoder_state_dict'])\n",
    "            optim.load_state_dict(checkpoint_dict['optimizer_state_dict'])\n",
    "            scheduler.load_state_dict(checkpoint_dict['scheduler_state_dict'])\n",
    "            print('Resuming from epoch %s'%(str(start_epoch)))     \n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        \n",
    "    if args['generative_model'] == 'autoencoder':\n",
    "        train_autoencoder_dataloader(dataloader_train, dataloader_val,\n",
    "                          device, model, optim, loss_fn,\n",
    "                          bsize = args['batch_size'],\n",
    "                          start_epoch = start_epoch,\n",
    "                          n_epochs = args['num_epochs'],\n",
    "                          eval_freq = args['eval_frequency'],\n",
    "                          scheduler = scheduler,\n",
    "                          writer = writer,\n",
    "                          save_recons=True,\n",
    "                          shapedata=shapedata,\n",
    "                          metadata_dir=checkpoint_path, samples_dir=samples_path,\n",
    "                          checkpoint_path = args['checkpoint_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.fc_latent_dec.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint from file /home/jingwang/Data/data/FaceWarehouse/results/spirals_autoencoder/latent_128/checkpoints/align_pose/checkpoint.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1e309aab8515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                                      \u001b[0mshapedata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworst_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworst_mesh_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                                      \u001b[0mworst_face_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'worst_face_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                                                      mm_constant = 100)    \n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/Data2/jingwang/git-task/Neural3DMM/test_funcs.pyc\u001b[0m in \u001b[0;36mtest_autoencoder_dataloader\u001b[0;34m(device, model, dataloader_test, shapedata, worst_face_num, worst_path, mm_constant, save_recons, samples_dir)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'points'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jingwang/Data/anaconda3/envs/py2-spiral/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jingwang/Data/anaconda3/envs/py2-spiral/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 raise RuntimeError(\"module must have its parameters and buffers \"\n\u001b[1;32m    145\u001b[0m                                    \u001b[0;34m\"on device {} (device_ids[0]) but found one of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                                    \"them on device: {}\".format(self.src_device_obj, t.device))\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu"
     ]
    }
   ],
   "source": [
    "if args['mode'] == 'test':\n",
    "    print('loading checkpoint from file %s'%(os.path.join(checkpoint_path,args['checkpoint_file']+'.pth.tar')))\n",
    "    checkpoint_dict = torch.load(os.path.join(checkpoint_path,args['checkpoint_file']+'.pth.tar'),map_location=device)\n",
    "    model.load_state_dict(checkpoint_dict['autoencoder_state_dict'])\n",
    "        \n",
    "    predictions, norm_l1_loss, l2_loss = test_autoencoder_dataloader(device, model, dataloader_test, \n",
    "                                                                     shapedata, worst_path=worst_mesh_path, \n",
    "                                                                     worst_face_num=args['worst_face_num'],\n",
    "                                                                     mm_constant = 100)    \n",
    "    np.save(os.path.join(prediction_path,'predictions'), predictions)   \n",
    "        \n",
    "    print('autoencoder: normalized loss', norm_l1_loss)\n",
    "    \n",
    "    print('autoencoder: euclidean distance in mm=', l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataParallel(\n",
      "  (module): SpiralAutoencoder(\n",
      "    (conv_0): SpiralConv(\n",
      "      (conv): Linear(in_features=42, out_features=16, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (conv_1): SpiralConv(\n",
      "      (conv): Linear(in_features=208, out_features=32, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (conv_2): SpiralConv(\n",
      "      (conv): Linear(in_features=288, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (conv_3): SpiralConv(\n",
      "      (conv): Linear(in_features=576, out_features=128, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (fc_latent_enc): Linear(in_features=5888, out_features=146, bias=True)\n",
      "    (fc_latent_dec): Linear(in_features=146, out_features=5888, bias=True)\n",
      "    (dconv): ModuleList(\n",
      "      (0): SpiralConv(\n",
      "        (conv): Linear(in_features=1152, out_features=64, bias=True)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (1): SpiralConv(\n",
      "        (conv): Linear(in_features=576, out_features=32, bias=True)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (2): SpiralConv(\n",
      "        (conv): Linear(in_features=416, out_features=32, bias=True)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (3): SpiralConv(\n",
      "        (conv): Linear(in_features=448, out_features=16, bias=True)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (4): SpiralConv(\n",
      "        (conv): Linear(in_features=224, out_features=3, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (dconv_0): SpiralConv(\n",
      "      (conv): Linear(in_features=1152, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (dconv_1): SpiralConv(\n",
      "      (conv): Linear(in_features=576, out_features=32, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (dconv_2): SpiralConv(\n",
      "      (conv): Linear(in_features=416, out_features=32, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (dconv_3): SpiralConv(\n",
      "      (conv): Linear(in_features=448, out_features=16, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (dconv_4): SpiralConv(\n",
      "      (conv): Linear(in_features=224, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "),\n",
      " SpiralAutoencoder(\n",
      "  (conv_0): SpiralConv(\n",
      "    (conv): Linear(in_features=42, out_features=16, bias=True)\n",
      "    (activation): ELU(alpha=1.0)\n",
      "  )\n",
      "  (conv_1): SpiralConv(\n",
      "    (conv): Linear(in_features=208, out_features=32, bias=True)\n",
      "    (activation): ELU(alpha=1.0)\n",
      "  )\n",
      "  (conv_2): SpiralConv(\n",
      "    (conv): Linear(in_features=288, out_features=64, bias=True)\n",
      "    (activation): ELU(alpha=1.0)\n",
      "  )\n",
      "  (conv_3): SpiralConv(\n",
      "    (conv): Linear(in_features=576, out_features=128, bias=True)\n",
      "    (activation): ELU(alpha=1.0)\n",
      "  )\n",
      "  (fc_latent_enc): Linear(in_features=5888, out_features=146, bias=True)\n",
      "  (fc_latent_dec): Linear(in_features=146, out_features=5888, bias=True)\n",
      "  (dconv): ModuleList(\n",
      "    (0): SpiralConv(\n",
      "      (conv): Linear(in_features=1152, out_features=64, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (1): SpiralConv(\n",
      "      (conv): Linear(in_features=576, out_features=32, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (2): SpiralConv(\n",
      "      (conv): Linear(in_features=416, out_features=32, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (3): SpiralConv(\n",
      "      (conv): Linear(in_features=448, out_features=16, bias=True)\n",
      "      (activation): ELU(alpha=1.0)\n",
      "    )\n",
      "    (4): SpiralConv(\n",
      "      (conv): Linear(in_features=224, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (dconv_0): SpiralConv(\n",
      "    (conv): Linear(in_features=1152, out_features=64, bias=True)\n",
      "    (activation): ELU(alpha=1.0)\n",
      "  )\n",
      "  (dconv_1): SpiralConv(\n",
      "    (conv): Linear(in_features=576, out_features=32, bias=True)\n",
      "    (activation): ELU(alpha=1.0)\n",
      "  )\n",
      "  (dconv_2): SpiralConv(\n",
      "    (conv): Linear(in_features=416, out_features=32, bias=True)\n",
      "    (activation): ELU(alpha=1.0)\n",
      "  )\n",
      "  (dconv_3): SpiralConv(\n",
      "    (conv): Linear(in_features=448, out_features=16, bias=True)\n",
      "    (activation): ELU(alpha=1.0)\n",
      "  )\n",
      "  (dconv_4): SpiralConv(\n",
      "    (conv): Linear(in_features=224, out_features=3, bias=True)\n",
      "  )\n",
      "),\n",
      " SpiralConv(\n",
      "  (conv): Linear(in_features=42, out_features=16, bias=True)\n",
      "  (activation): ELU(alpha=1.0)\n",
      "),\n",
      " Linear(in_features=42, out_features=16, bias=True),\n",
      " ELU(alpha=1.0),\n",
      " SpiralConv(\n",
      "  (conv): Linear(in_features=208, out_features=32, bias=True)\n",
      "  (activation): ELU(alpha=1.0)\n",
      "),\n",
      " Linear(in_features=208, out_features=32, bias=True),\n",
      " ELU(alpha=1.0),\n",
      " SpiralConv(\n",
      "  (conv): Linear(in_features=288, out_features=64, bias=True)\n",
      "  (activation): ELU(alpha=1.0)\n",
      "),\n",
      " Linear(in_features=288, out_features=64, bias=True),\n",
      " ELU(alpha=1.0),\n",
      " SpiralConv(\n",
      "  (conv): Linear(in_features=576, out_features=128, bias=True)\n",
      "  (activation): ELU(alpha=1.0)\n",
      "),\n",
      " Linear(in_features=576, out_features=128, bias=True),\n",
      " ELU(alpha=1.0),\n",
      " Linear(in_features=5888, out_features=146, bias=True),\n",
      " Linear(in_features=146, out_features=5888, bias=True),\n",
      " ModuleList(\n",
      "  (0): SpiralConv(\n",
      "    (conv): Linear(in_features=1152, out_features=64, bias=True)\n",
      "    (activation): ELU(alpha=1.0)\n",
      "  )\n",
      "  (1): SpiralConv(\n",
      "    (conv): Linear(in_features=576, out_features=32, bias=True)\n",
      "    (activation): ELU(alpha=1.0)\n",
      "  )\n",
      "  (2): SpiralConv(\n",
      "    (conv): Linear(in_features=416, out_features=32, bias=True)\n",
      "    (activation): ELU(alpha=1.0)\n",
      "  )\n",
      "  (3): SpiralConv(\n",
      "    (conv): Linear(in_features=448, out_features=16, bias=True)\n",
      "    (activation): ELU(alpha=1.0)\n",
      "  )\n",
      "  (4): SpiralConv(\n",
      "    (conv): Linear(in_features=224, out_features=3, bias=True)\n",
      "  )\n",
      "),\n",
      " SpiralConv(\n",
      "  (conv): Linear(in_features=1152, out_features=64, bias=True)\n",
      "  (activation): ELU(alpha=1.0)\n",
      "),\n",
      " Linear(in_features=1152, out_features=64, bias=True),\n",
      " ELU(alpha=1.0),\n",
      " SpiralConv(\n",
      "  (conv): Linear(in_features=576, out_features=32, bias=True)\n",
      "  (activation): ELU(alpha=1.0)\n",
      "),\n",
      " Linear(in_features=576, out_features=32, bias=True),\n",
      " ELU(alpha=1.0),\n",
      " SpiralConv(\n",
      "  (conv): Linear(in_features=416, out_features=32, bias=True)\n",
      "  (activation): ELU(alpha=1.0)\n",
      "),\n",
      " Linear(in_features=416, out_features=32, bias=True),\n",
      " ELU(alpha=1.0),\n",
      " SpiralConv(\n",
      "  (conv): Linear(in_features=448, out_features=16, bias=True)\n",
      "  (activation): ELU(alpha=1.0)\n",
      "),\n",
      " Linear(in_features=448, out_features=16, bias=True),\n",
      " ELU(alpha=1.0),\n",
      " SpiralConv(\n",
      "  (conv): Linear(in_features=224, out_features=3, bias=True)\n",
      "),\n",
      " Linear(in_features=224, out_features=3, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(list(model.modules()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
